---
title: "vignette.qmd"
format: html
editor: visual
date: today
author: Mindy Xu, Zoe Zhou, Amy Lyu, Jerry Huang
---

# Random Forest Vignette

## Introduction

> In this project, we are presenting a vignette on Random Forest. If you are new to this topic, then this vignette would be a great guideline for you to have a great understanding on Random forest model, and we will also show the sample code on a dataset to predict the edibility of mushrooms using Random Forest.

*Learning Objectives:*

-   learn the intuitions behind Random Forest models

    -   Decision trees, hyperparameters, and more

-   Implement, tune, and evaluate Random Forest models in R

Here we have a introductory video for you to better know what is Random Forest.

[Introductory Video](https://www.youtube.com/watch?v=v6VJ2RO66Ag)

## Example Data -- Mushroom Data to Implement

### Data Description

```         
This dataset includes descriptions of hypothetical samples of 23 species of mushrooms. It contains 22 categorical predictor variables and one target variable. The objective is to use different attributes of the mushrooms to make the most accurate predictions of the edibility of them. The samples are hypothetical because the dataset was designed in a way so that no simple decision rules may apply. There are 8124 samples in total.
```

### Load Packages and data

```{r results='hide', message=FALSE}
# loading packages
library(tidymodels)
library(tidyverse)
library(randomForest)
library(dplyr)
library(caret)
library(janitor)
library(ranger)
tidymodels_prefer()


# loading the data
mushrooms <- read.csv('data/mushrooms.csv')
```

### Preprocess the data

```{r}         
# turn a categorical variable into factors
mushrooms$class <- as.factor(mushrooms$class)
```

::: {.callout-note style="background-color: #ffebee; color: black; border-left: 3px solid red;"}
#### Action

Turn other categorical variables into factors.
:::

## Data Partition

```{r}         
set.seed(10000)
# Splitting the data (70/30 split, stratify on capture rate)
mushrooms_split <- initial_split(mushrooms, prop = 0.7, strata = class)
mushrooms_train <- training(mushrooms_split)
mushrooms_test <- testing(mushrooms_split)
```

::: {.callout-note style="background-color: #ffebee; color: black; border-left: 3px solid red;"}
#### Action

Try split the data with different stratification to see if the results change.
:::

## Intuition and Architecture

### Understanding Decision Trees

```         
/* Description:
/* Sample code: 
/* Action:
```

### Random Forest and Ensemble models

```         
/* Description:
/* Sample code: Build a random forest model
/* Action: Try altering the number of trees / max depths of trees / etc and assess the model's performance.
```

## Hyperparameters Tuning

### Random Forest Parameters overview

### Grid/Cross-validation

```         
/* Description
/* Sample code: tune one parameter
/* Action: tune other parameters using a grid
```

## Understanding the Random Forest model

### Features Importance

```         
/* Description
/* Sample code
```

> Random Forest Algorithm widespread popularity stems from its user-friendly nature and adaptability, enabling it to tackle both classification and regression problems effectively. The algorithm's strength lies in its ability to handle complex datasets and mitigate overfitting, making it a valuable tool for various predictive tasks in machine learning.

> One of the most important features of the Random Forest Algorithm is that it can handle the data set containing **continuous variables**, as in the case of **regression**, and **categorical variables**, as in the case of **classification**. It performs better for classification and regression tasks. In this tutorial, we will understand the working of random forest and implement random forest on a classification task.

![](image/img-rfsimplified.png)

#### Steps Involved in Random Forest Algorithm

-   Step 1：In the Random forest model, a subset of data points and a subset of features is selected for constructing each decision tree. Simply put, n random records and m features are taken from the data set having k number of records.
-   Step 2：Individual decision trees are constructed for each sample.
-   Step 3：Each decision tree will generate an output.
-   Step 4：Final output is considered based on ***Majority Voting or Averaging*** for Classification and regression, respectively.

#### Important Features of Random Forest

-   **Ensemble of Decision Trees：** Random Forest is an ensemble learning method, which means it combines the predictions from multiple machine learning algorithms to make more accurate predictions than any individual model. Specifically, it builds multiple decision trees and merges them together to get a more accurate and stable prediction.
-   **Handling of Both Categorical and Numerical Features：**It can handle a mix of categorical and numerical features. There is no need to pre-process data to convert categorical features to numerical features.
-   **Feature Importance：**One of the useful outputs of Random Forest is the importance or contribution of each feature in the prediction. This helps in understanding the data better and can be used for feature selection.
-   **Avoids Overfitting：** Due to the way it constructs the decision trees (using a subset of features and samples), it generally does a good job of avoiding overfitting, especially compared to individual decision trees.
-   **No Need for Future Scaling：**Random Forest does not require feature scaling (like standardization or normalization) before input, as it does not rely on distance calculations.
-   **Handles Missing Values:** It can handle missing values in the data, though the way it does this can vary depending on the implementation.
-   **Robust to Outliers:** It is generally robust to outliers and can handle them better than many other algorithms.
-   **Good for Large Datasets：**It can handle large datasets with higher dimensionality (many features) and can evaluate the importance of different features for the classification/regression tasks.
-   **Versatility in Performance Metrics：** It supports various metrics for evaluating the performance suitable for different types of problems (classification, regression)
-   **Parallelizable：** The algorithm can be parallelized for execution because each tree in the forest is built independently of the others, which makes the algorithm well-suited for modern multi-processor computers.

## Example Data -- Mushroom Data to Implement

#### Data Description

> This dataset includes descriptions of hypothetical samples of 23 species of mushrooms. It contains 22 categorical predictor variables and one target variable. The objective is to use different attributes of the mushrooms to make the most accurate predictions of the edibility of them. The samples are hypothetical because the dataset was designed in a way so that no simple decision rules may apply. There are 8124 samples in total.

##### Load Library

```{r results='hide', message=FALSE}
library(tidymodels)
library(tidyverse)
library(randomForest)
library(dplyr)
library(caret)
library(janitor)
library(ranger)
tidymodels_prefer()
```

##### Load Data

```{r}
# loading the data
mushrooms <- read.csv('data/mushrooms.csv')
```

##### Preliminary Task

```{r}
# cleaning predictor names
mushrooms <- clean_names(mushrooms)
```
